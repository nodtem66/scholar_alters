{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Found 0 messages\n",
      "Found 0 papers (New: 0, Duplicated: 0)\n",
      "Unread: 29\n"
     ]
    }
   ],
   "source": [
    "# Get emails from Gmail and parse the papers and save it as excel file\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from connect_to_service import *\n",
    "from parse_gmail_message import *\n",
    "\n",
    "import pandas as pd\n",
    "import base64\n",
    "import os.path as ospath\n",
    "\n",
    "# Override the default parameters\n",
    "DATA_FOLDER = \"./data/\"\n",
    "PAPERS_LABEL = 'Subscribe/Gscholar'\n",
    "SHEET_ID = '1Z5Riim21O7Ti5hHlWzriBhn2CbJibqee4psHkonCcBw'\n",
    "\n",
    "# Create data folder if not exists\n",
    "if not ospath.exists(DATA_FOLDER):\n",
    "  ospath.makedirs(DATA_FOLDER)\n",
    "\n",
    "# Call the Gmail API\n",
    "creds = get_creds(DATA_FOLDER)\n",
    "service = get_gmail_service(creds)\n",
    "sheet_service = get_sheets_service(creds)\n",
    "\n",
    "# Get all the messages with labels\n",
    "labels = GetLabelsId(service,'me',[PAPERS_LABEL,'UNREAD'])\n",
    "messages = ListMessagesWithLabels(service,\"me\",labels)\n",
    "print (f'Found {len(messages)} messages')\n",
    "\n",
    "# Parse the mails\n",
    "pa = PaperAggregator()\n",
    "\n",
    "for msg in messages:\n",
    "  msg_content = GetMessage(service, \"me\", msg['id'])\n",
    "  try:\n",
    "    msg_str = base64.urlsafe_b64decode(msg_content['payload']['body']['data'].encode('utf-8'))\n",
    "  except KeyError:\n",
    "    continue\n",
    "\n",
    "  msg_title = ''\n",
    "  for h in msg_content['payload']['headers']:\n",
    "    if h['name'] == 'Subject':\n",
    "      msg_title = h['value']\n",
    "  parser = PapersHTMLParser(msg_title)\n",
    "  parser.feed(str(msg_str))\n",
    "\n",
    "  for paper in parser.papers:\n",
    "    pa.add(paper)\n",
    "\n",
    "total_papers = len(pa.paper_list)\n",
    "# Remove previously seen papers\n",
    "old_pa = PaperAggregator()\n",
    "old_pa.load_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'))\n",
    "#old_pa.load_csv(ospath.join(DATA_FOLDER, 'archive.csv'))\n",
    "for paper in old_pa.paper_list:\n",
    "  pa.remove(paper)\n",
    "\n",
    "# Sort by number of refernece mails\n",
    "total_new_papers = len(pa.paper_list)\n",
    "print (f'Found {total_papers} papers (New: {total_new_papers}, Duplicated: {total_papers-total_new_papers})')\n",
    "\n",
    "old_pa.merge(pa)\n",
    "df = old_pa.to_dataframe()\n",
    "df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)\n",
    "#df.to_csv(ospath.join(DATA_FOLDER, 'archive.csv'), index=False)\n",
    "print (f'Unread: {df.query(\"status == 0\").shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unread: 29\n",
      "Querying TLDR for 0 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown TLDR: 0\n"
     ]
    }
   ],
   "source": [
    "# Query TLDR from semantic scholar\n",
    "import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "get_datetime = lambda: datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = Path('./semantic_scholar_api_key.txt').read_text().strip()\n",
    "\n",
    "def query_tldr_all_papers(df):\n",
    "  #new_papers = [datetime.fromisoformat(d2) - datetime.fromisoformat(d1) < timedelta(days=14) for (_, d1, d2) in df[['created_at', 'updated_at']].itertuples() ]\n",
    "  #new_papers = pd.Series(new_papers)\n",
    "  query_df = df[df['tldr'].isnull() & (df['status'] == 0)]\n",
    "  print(f\"Querying TLDR for {query_df.shape[0]} papers\")\n",
    "  titles = [\n",
    "    re.sub('[^A-Za-z0-9 ]+', ' ', title.strip())\n",
    "    for title in query_df.title\n",
    "  ]\n",
    "  urls = [\n",
    "    f\"https://api.semanticscholar.org/graph/v1/paper/search?query={urllib.parse.quote_plus(title)}&fields=tldr,abstract&limit=1\"\n",
    "    for title in titles\n",
    "  ]\n",
    "  \n",
    "  index_loc = query_df.index.to_list()\n",
    "  assert len(index_loc) == len(urls), 'Index length mismatch'\n",
    "  \n",
    "  for i in tqdm(range(len(urls))):\n",
    "    tstart = time.time()\n",
    "    response = requests.get(urls[i], headers={'x-api-key': SEMANTIC_SCHOLAR_API_KEY})\n",
    "    tend = time.time()\n",
    "    if response.status_code == 200:\n",
    "      response = response.json()\n",
    "      if response.get('total') > 0:\n",
    "        data = response.get('data')\n",
    "        if data and len(data) > 0:\n",
    "          result = data[0]\n",
    "          tldr = result.get('tldr')\n",
    "          if tldr and tldr.get('text'):\n",
    "            df.loc[index_loc[i], 'tldr'] = tldr.get('text')\n",
    "    # Update the time\n",
    "    df.loc[index_loc[i], 'updated_at'] = get_datetime()\n",
    "\n",
    "    # # Sleep for 1 second to avoid rate limit\n",
    "    time.sleep(max(0, 1 - (tend - tstart)))\n",
    "\n",
    "print (f'Unread: {df.query(\"status == 0\").shape[0]}')\n",
    "query_tldr_all_papers(df)\n",
    "df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)\n",
    "\n",
    "print(f\"Unknown TLDR: {df[df['tldr'].isnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Updated records: 0\n",
      "Number of New records: 0\n"
     ]
    }
   ],
   "source": [
    "# Update google sheet\n",
    "prepare_data = PrepareDataforUpdateSheet(sheet_service, SHEET_ID, df)\n",
    "print(f\"Number of Updated records: {len(prepare_data['write_data'])}\")\n",
    "print(f\"Number of New records: {len(prepare_data['append_data'])}\")\n",
    "if len(prepare_data['write_data']) > 0:\n",
    "  UpdateSheet(sheet_service, SHEET_ID, prepare_data['write_data'])\n",
    "if len(prepare_data['append_data']) > 0:\n",
    "  AppendSheet(sheet_service, SHEET_ID, prepare_data['append_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all as read\n",
    "body = {\"addLabelIds\": [], \"removeLabelIds\": [\"UNREAD\",\"INBOX\"]}\n",
    "for msg in messages:\n",
    "  service.users().messages().modify(userId=\"me\", id=msg['id'], body=body).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the papers\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def print_html(html):\n",
    "  display(HTML(html))\n",
    "\n",
    "def print_markdown(md):\n",
    "  display(Markdown(md))\n",
    "\n",
    "unread_df = df.query(\"status == 0\")\n",
    "item_per_page = 10\n",
    "total_new_papers = unread_df.shape[0] \n",
    "total_page = total_new_papers // item_per_page + 1\n",
    "\n",
    "# Change page index here \n",
    "# start from zero \n",
    "if 'page' in globals():\n",
    "  page = page + 1 # type: ignore\n",
    "else:\n",
    "  page = 0\n",
    "\n",
    "counter = 1\n",
    "print_html(f'<h3 style=\"color:yellow\">Page {page+1}/{total_page}<br><small>Item / page: {item_per_page}</small></h3>')\n",
    "for i,row in unread_df[page*item_per_page:(page+1)*item_per_page].iterrows():\n",
    "  print_html(f'<h3><input type=\"checkbox\" style=\"float:left\"/> {page*item_per_page+counter} / {total_new_papers}</h3>')\n",
    "  print_html(f'<div><b style=\"color:lightblue\">{row.title}</b></div>')\n",
    "  if isinstance(row.tldr, str) and len(row.tldr) > 0:\n",
    "    print_html(f'<div>[TLDR]: {row.tldr}</div>')\n",
    "  else:\n",
    "    print_html(f'<div>{row.data}</div>')\n",
    "  print_html(f'<div>Auhtors: {row.authors}<br><cite>Email title: {row.email_title}</cite><br><a href=\"{row.link}\">Link</a></div>')\n",
    "  counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'page' in globals():\n",
    "  read_df = unread_df[:(page)*item_per_page]\n",
    "  print(f'Saved {len(read_df)} read papers to excel')\n",
    "  df.loc[read_df.index, 'status'] = 1\n",
    "  df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)\n",
    "else:\n",
    "  print('No papers read')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gscholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
