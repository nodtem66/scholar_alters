{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emails from Gmail and parse the papers\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from connect_to_gmail import *\n",
    "from parse_gmail_message import *\n",
    "import pandas as pd\n",
    "\n",
    "# Override the default parameters\n",
    "DATA_FOLDER = r'./data'\n",
    "PAPERS_LABEL = 'Subscribe/Gscholar'\n",
    "\n",
    "# Create data folder if not exists\n",
    "if not ospath.exists(DATA_FOLDER):\n",
    "  makedirs(DATA_FOLDER)\n",
    "\n",
    "# Call the Gmail API\n",
    "service = get_service(DATA_FOLDER)\n",
    "\n",
    "# Get all the messages with labels\n",
    "labels = GetLabelsId(service,'me',[PAPERS_LABEL,'UNREAD'])\n",
    "messages = ListMessagesWithLabels(service,\"me\",labels)\n",
    "print (f'Found {len(messages)} messages')\n",
    "\n",
    "# Parse the mails\n",
    "pa = PaperAggregator()\n",
    "\n",
    "for msg in messages:\n",
    "  msg_content = GetMessage(service, \"me\", msg['id'])\n",
    "  try:\n",
    "    msg_str = base64.urlsafe_b64decode(msg_content['payload']['body']['data'].encode('utf-8'))\n",
    "  except KeyError:\n",
    "    continue\n",
    "\n",
    "  msg_title = ''\n",
    "  for h in msg_content['payload']['headers']:\n",
    "    if h['name'] == 'Subject':\n",
    "      msg_title = h['value']\n",
    "  parser = PapersHTMLParser(msg_title)\n",
    "  parser.feed(str(msg_str))\n",
    "\n",
    "  for paper in parser.papers:\n",
    "    pa.add(paper)\n",
    "\n",
    "total_papers = len(pa.paper_list)\n",
    "# Remove previously seen papers\n",
    "old_pa = PaperAggregator()\n",
    "old_pa.load_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'))\n",
    "for paper in old_pa.paper_list:\n",
    "  pa.remove(paper)\n",
    "\n",
    "# Sort by number of refernece mails\n",
    "total_new_papers = len(pa.paper_list)\n",
    "print (f'Found {total_papers} papers (New: {total_new_papers}, Duplicated: {total_papers-total_new_papers})')\n",
    "\n",
    "old_pa.merge(pa)\n",
    "df = old_pa.to_dataframe()\n",
    "df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)\n",
    "print (f'Unread: {df.query(\"status == 0\").shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all as read\n",
    "body = {\"addLabelIds\": [], \"removeLabelIds\": [\"UNREAD\",\"INBOX\"]}\n",
    "for msg in messages:\n",
    "  service.users().messages().modify(userId=\"me\", id=msg['id'], body=body).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query TLDR from semantic scholar\n",
    "import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = Path('./semantic_scholar_api_key.txt').read_text().strip()\n",
    "\n",
    "# Parallel query\n",
    "# Since the API is rate limited (1 request per seconds), we can't use this code\n",
    "# async def fetch(session, url):\n",
    "#   async with session.get(url) as response:\n",
    "#     return await response.json()\n",
    "\n",
    "\n",
    "# async def query_tldr_all_papers(papers):\n",
    "#   titles = [\n",
    "#     re.sub('[^A-Za-z0-9 ]+', ' ', paper.title.strip())\n",
    "#     for paper in papers\n",
    "#   ]\n",
    "#   urls = [\n",
    "#     f\"https://api.semanticscholar.org/graph/v1/paper/search?query={urllib.parse.quote_plus(title)}&fields=tldr&limit=1\"\n",
    "#     for title in titles\n",
    "#   ]\n",
    "\n",
    "#   async with aiohttp.ClientSession() as session:\n",
    "#     tasks = [asyncio.ensure_future(fetch(session, url)) for url in urls]\n",
    "#     responses = await asyncio.gather(*tasks)\n",
    "#     for i,response in enumerate(responses):\n",
    "#       print(response)\n",
    "#       if response.get('total') > 0:\n",
    "#         data = response.get('data')\n",
    "#         if data and len(data) > 0:\n",
    "#           result = data[0]\n",
    "#           tldr = result.get('tldr')\n",
    "#           if tldr:\n",
    "#             papers[i].tldr = tldr.get('text')\n",
    "\n",
    "def query_tldr_all_papers(df):\n",
    "  query_df = df[df['tldr'].isnull()]\n",
    "  print(f\"Querying TLDR for {query_df.shape[0]} papers\")\n",
    "  titles = [\n",
    "    re.sub('[^A-Za-z0-9 ]+', ' ', title.strip())\n",
    "    for title in query_df.title\n",
    "  ]\n",
    "  urls = [\n",
    "    f\"https://api.semanticscholar.org/graph/v1/paper/search?query={urllib.parse.quote_plus(title)}&fields=tldr,abstract&limit=1\"\n",
    "    for title in titles\n",
    "  ]\n",
    "  \n",
    "  index_loc = query_df.index.to_list()\n",
    "  assert len(index_loc) == len(urls), 'Index length mismatch'\n",
    "  \n",
    "  for i in tqdm(range(10)):\n",
    "    tstart = time.time()\n",
    "    response = requests.get(urls[i], headers={'x-api-key': SEMANTIC_SCHOLAR_API_KEY})\n",
    "    tend = time.time()\n",
    "    if response.status_code == 200:\n",
    "      response = response.json()\n",
    "      if response.get('total') > 0:\n",
    "        data = response.get('data')\n",
    "        if data and len(data) > 0:\n",
    "          result = data[0]\n",
    "          tldr = result.get('tldr')\n",
    "          if tldr and tldr.get('text'):\n",
    "            df.loc[index_loc[i], 'tldr'] = tldr.get('text')\n",
    "\n",
    "    # # Sleep for 1 second to avoid rate limit\n",
    "    time.sleep(max(0, 1 - (tend - tstart)))\n",
    "\n",
    "query_tldr_all_papers(df)\n",
    "df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)\n",
    "\n",
    "print(f\"Unknown TLDR: {df[df['tldr'].isnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the papers\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def print_html(html):\n",
    "  display(HTML(html))\n",
    "\n",
    "def print_markdown(md):\n",
    "  display(Markdown(md))\n",
    "\n",
    "unread_df = df.query(\"status == 0\")\n",
    "item_per_page = 10\n",
    "total_new_papers = unread_df.shape[0] \n",
    "total_page = total_new_papers // item_per_page + 1\n",
    "\n",
    "# Change page index here \n",
    "# start from zero \n",
    "page = 8\n",
    "\n",
    "counter = 1\n",
    "print_html(f'<h3 style=\"color:yellow\">Page {page+1}/{total_page}<br><small>Item / page: {item_per_page}</small></h3>')\n",
    "for i,row in unread_df[page*item_per_page:(page+1)*item_per_page].iterrows():\n",
    "  print_html(f'<h3><input type=\"checkbox\" style=\"float:left\"/> {page*item_per_page+counter} / {total_new_papers}</h3>')\n",
    "  print_html(f'<div><b style=\"color:lightblue\">{row.title}</b></div>')\n",
    "  print_html(f'<div>{row.tldr}</div>')\n",
    "  print_html(f'<div>Auhtors: {row.authors}<br><cite>Email title: {row.email_title}</cite><br><a href=\"{row.link}\">Link</a></div>')\n",
    "  counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark as read and save to excel\n",
    "df.loc[unread_df.index, 'status'] = 1\n",
    "df.to_excel(ospath.join(DATA_FOLDER, 'archive.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gscholar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
